---
---

@string{aps = {American Physical Society,}}

@article{li2025coalignment,
  abbr={Agents4Science},
  title={Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation},
  author={Li, Yubo and Song, Weiyi},
  journal={arXiv preprint arXiv:2509.12179},
  year={2025},
  month={September},
  url={https://openreview.net/forum?id=G5jK2OMT2q#discussion},
  doi={10.48550/arXiv.2509.12179},
  html={https://openreview.net/forum?id=G5jK2OMT2q#discussion},
  pdf={co_alignment.pdf},
  preview={a4s.png},
  abstract={Current AI alignment through RLHF follows a single directional paradigm that AI conforms to human preferences while treating human cognition as fixed. We propose a shift to co-alignment through Bidirectional Cognitive Alignment (BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols, representation mapping, and KL-budget constraints for controlled co-evolution. In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline, with 230% better mutual adaptation and 332% better protocol convergence.}
}

@article{li2025time,
  abbr={arXiv},
  title={Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks},
  author={Li, Yubo and Krishnan, Ramayya and Padman, Rema},
  journal={arXiv preprint arXiv:2510.02712},
  year={2025},
  month={October},
  url={https://arxiv.org/abs/2510.02712},
  doi={10.48550/arXiv.2510.02712},
  pdf={time_to.pdf},
  abstract={Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present the first comprehensive survival analysis of conversational AI robustness, analyzing 36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a time-to-event process.}
}

@article{li2025chain,
  abbr={arXiv},
  title={Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modelings},
  author={Li, Yubo and Padman, Rema},
  journal={arXiv preprint arXiv:2510.09895},
  year={2025},
  month={October},
  url={https://arxiv.org/abs/2510.09895},
  doi={10.48550/arXiv.2510.09895},
  preview={coi.png},
  abstract={Modeling clinical time-series data is hampered by the challenge of capturing latent, time-varying dependencies among features. State-of-the-art approaches often rely on black-box mechanisms or simple aggregation, failing to explicitly model how the influence of one clinical variable propagates through others over time. We propose Chain-of-Influence (CoI), an interpretable deep learning framework that constructs an explicit, time-unfolded graph of feature interactions.}
}

@article{li2025no,
  abbr={AMIA},
  title={No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism},
  author={Li, Yubo and Yao, Xinyu and Padman, Rema},
  journal={arXiv preprint arXiv:2503.19285},
  year={2025},
  month={March},
  selected={true},
  url={https://arxiv.org/abs/2503.19285},
  preview={no_black.png},
  abstract={Despite the outstanding performance of deep learning models in clinical prediction tasks, explainability remains a significant challenge. Inspired by transformer architectures, we introduce the Temporal-Feature Cross Attention Mechanism (TFCAM), a novel deep learning framework designed to capture dynamic interactions among clinical features across time, enhancing both predictive accuracy and interpretability. In an experiment with 1,422 patients with Chronic Kidney Disease, predicting progression to End-Stage Renal Disease, TFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an F1-score of 0.69. Beyond performance gains, TFCAM provides multi-level explainability by identifying critical temporal periods, ranking feature importance, and quantifying how features influence each other across time before affecting predictions. Our approach addresses the "black box" limitations of deep learning in healthcare, offering clinicians transparent insights into disease progression mechanisms while maintaining state-of-the-art predictive performance.}
}

@article{li2025enhancing,
  abbr={JAMIA},
  title={Enhancing End-Stage Renal Disease Outcome Prediction: A Multisourced Data-Driven Approach},
  author={Li, Yubo and Padman, Rema},
  journal={Journal of the American Medical Informatics Association},
  year={2025},
  month={Aug},
  doi={10.1093/jamia/ocaf118},
  url={https://pubmed.ncbi.nlm.nih.gov/40795063/},
  preview={jamia.png},
  abstract={Objective: To improve prediction of Chronic Kidney Disease progression to End Stage Renal Disease using machine
    learning and deep learning models applied to integrated clinical and claims data with varying observation windows,
    supported by explainable AI to enhance interpretability and reduce bias.
    Materials and Methods: We utilized data from 10,326 CKD patients, combining clinical and claims information
    from 2009-2018. After preprocessing, cohort identification, and feature engineering, we evaluated multiple statistical,
    ML and DL models using five distinct observation windows. Feature importance and SHAP analysis were employed
    to understand key predictors. Models were tested for robustness, clinical relevance, misclassification patterns, and
    bias.
    Results: Integrated data models outperformed single data source models, with Long Short-Term Memory (LSTM)
    achieving the highest AUC (0.93) and F1 score (0.65). A 24-month observation window optimally balanced early
    detection and prediction accuracy. The 2021 eGFR equation improved prediction accuracy and reduced racial bias,
    particularly for African American patients.
    Discussion: Improved prediction accuracy, interpretability and bias mitigation strategies have the potential to enhance
    CKD management, support targeted interventions, and reduce healthcare disparities.
    Conclusion: This study presents a robust framework for predicting ESRD outcomes, improving clinical decision-
    making through integrated multi-sourced data and advanced analytics. Future research will expand data integration
    and extend this framework to other chronic diseases.},
  pdf={jamia.pdf},
  selected={true},
  note={Online ahead of print}
}

@inproceedings{li2024towards,
  abbr={AMIA},
  title={Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques},
  author={Li, Yubo and Al-Sayouri, Saba and Padman, Rema},
  booktitle={AMIA Annual Symposium Proceedings},
  year={2024},
  month={May},
  volume={2024},
  pages={664--673},
  pmid={40417492},
  pmcid={PMC12099416},
  html={https://pmc.ncbi.nlm.nih.gov/articles/PMC12099416/},
  pdf={towards.pdf},
  preview={towards.png},
  abstract={This study explores the potential of utilizing administrative claims data, combined with advanced machine learning and deep learning techniques, to predict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal Disease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major health insurance organization to develop prediction models for multiple observation windows using traditional machine learning methods such as Random Forest and XGBoost as well as deep learning approaches such as Long Short-Term Memory (LSTM) networks.}
}

@article{li2025beyond,
  abbr={arXiv},
  title={Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models},
  author={Li, Yubo and Shen, Xiaobin and Yao, Xinyu and Ding, Xueying and Miao, Yidi and Krishnan, Ramayya and Padman, Rema},
  journal={arXiv preprint arXiv:2504.04717},
  year={2025},
  month={April},
  doi = {10.48550/arXiv.2504.04717},
  code = {https://github.com/yubol-bobo/Awesome-Multi-Turn-LLMs},
  pdf = {beyond.pdf},
  selected={true},
  preview={beyond.png},
  abstract = {Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios—from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings—we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs.}
}

@inproceedings{ACL,
    abbr={ACL},
    title = "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions",
    author = "Li, Yubo  and
      Miao, Yidi  and
      Ding, Xueying  and
      Krishnan, Ramayya  and
      Padman, Rema",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = July,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = {https://aclanthology.org/2025.findings-acl.347/},
    code = {https://github.com/yubol-bobo/MT-Consistency},
    pdf = {firm_fickle.pdf},
    doi = "10.18653/v1/2025.findings-acl.347",
    preview={firm_fickle.png},
    pages = "6679--6700",
    selected={true},
    ISBN = "979-8-89176-256-5",
    abstract = "Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent and coherent behavior across multiple rounds of user interaction. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions . First, we introduce Position-Weighted Consistency (PWC), a metric designed to capture both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present MT-Consistency, a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by explicitly integrating internal model confidence scores during the generation process. Experimental results demonstrate that CARG significantly improves response stability without sacrificing accuracy, offering a practical path toward more dependable LLM behavior in critical, real-world deployments."
}






